<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/pure-min.css" integrity="sha384-nn4HPE8lTHyVtfCBi5yW9d20FjT8BJwUXyWZT9InLYax14RDjBj46LmSztkmNP9w" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/base-min.css">
    <link href="https://fonts.googleapis.com/css?family=Alegreya" rel="stylesheet">
</head>
<body>
<div class="main">
    <div class="pure-g">

        <div class="card pure-u-5-8 paper">
            <div class="box-container">
                <h3><b>Matching on Balanced Nonlinear Representations for Treatment Effects Estimation</b></h3>
                <p class="author">Sheng Li and Yun Fu</p>
                <a href="https://papers.nips.cc/paper/6694-matching-on-balanced-nonlinear-representations-for-treatment-effects-estimation.pdf">[PDF]</a>
                <p class="abstract">
                    Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classification problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be significantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.
                </p>
            </div>
        </div>


        <div class="pure-u-1-24">
        </div>
        <div class="card pure-u-5-24">
            <div class="box-container">
                <h3><b>Discussion Board</b></h3>
                <i>Excerpt from reddit.com/r/MachineLearning/</i>
                <div class="container">
                    <p>Not about writing, but I do have a question about submission strategy:

                        I will soon have a complete paper that I would be comfortable submitting to a conference. However, I think it may benefit from first presenting it at a workshop (for the reasons described in the OP); though it's also quite possible there will be minimal changes. I understand this is OK, because workshops are non-archival, but I worry that doing this in two steps will compromise anonymity for the double-blind conference submission later on. Is this a concern? Should I be skipping the workshop if I feel like my paper is already up to conference quality?</p>
                </div>

                <div class="container darker">
                    <p>I have a feeling going to those lengths to protect anonymity is unnessecary. Recently, we saw authors directly link to a github with their names listed and the paper was still accepted (albeit controversially).</p>
                </div>


                <div class="pure-form">
                    <input type="text" class="pure-input-rounded">
                    <button type="submit" class="pure-button">Enter</button>
                </div>
            </div>
        </div>
    </div>
</div>
</body>
</html>