<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/pure-min.css" integrity="sha384-nn4HPE8lTHyVtfCBi5yW9d20FjT8BJwUXyWZT9InLYax14RDjBj46LmSztkmNP9w" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/base-min.css">
    <link href="https://fonts.googleapis.com/css?family=Alegreya" rel="stylesheet">
</head>
<body>
<div class="main">
    <div class="pure-g">

        <div class="card pure-u-5-8 paper">
            <div class="box-container">
                <h3><b>The Nature of Statistical Learning Theory</b></h3>
                <p class="author"> Vladimir N. Vapnik</p>
                <a href="https://www.springer.com/br/book/9780387987804">[Link]</a>
                <p class="abstract">
                    The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists.

                </p>
            </div>
        </div>


        <div class="pure-u-1-24">
        </div>
        <div class="card pure-u-5-24">
            <div class="box-container">
                <h3><b>Discussion Board</b></h3>
                <i>Excerpt from reddit.com/r/MachineLearning/</i>
                <div class="container">
                    <p>Not about writing, but I do have a question about submission strategy:

                        I will soon have a complete paper that I would be comfortable submitting to a conference. However, I think it may benefit from first presenting it at a workshop (for the reasons described in the OP); though it's also quite possible there will be minimal changes. I understand this is OK, because workshops are non-archival, but I worry that doing this in two steps will compromise anonymity for the double-blind conference submission later on. Is this a concern? Should I be skipping the workshop if I feel like my paper is already up to conference quality?</p>
                </div>

                <div class="container darker">
                    <p>I have a feeling going to those lengths to protect anonymity is unnessecary. Recently, we saw authors directly link to a github with their names listed and the paper was still accepted (albeit controversially).</p>
                </div>


                <div class="pure-form">
                    <input type="text" class="pure-input-rounded">
                    <button type="submit" class="pure-button">Enter</button>
                </div>
            </div>
        </div>
    </div>
</div>
</body>
</html>